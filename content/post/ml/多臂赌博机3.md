---
title: "多臂赌博机3"
date: 2014-10-11T14:08:52+08:00
tags: ["MAB"]
categories: ["ML"]
toc: true
---

在上两节我们讨论的UCB系列算法面对的情况是静态的,即各臂的分布参数不会改变,于是我们就"乐观地面对不确定性"--根据采样平均值尽快地确定那个最好的臂.

但是在现实世界中收益结构是更复杂的,非静态的.特别是当它涉及到竞争的场景,如股票交易.我们称之为`对抗模式多臂赌博机(adversarial bandit)`

## 问题
数学化的问题描述可以参考[第一节](http://mlyixi.byethost32.com/blog/?p=155).

但是对于奖励矢量,有些异同:

* 它和静态时一样,必须是预先固定的.
* 各臂奖励期望随着轮数是变化的(准确地说是没有期望,或者说期望被对手动了手脚)

这时我们可以思考这种情况:对手能否总把参与者下一次的收益设为零?

## 定义
回想之前的UCB策略形成过程,我们的目标是尽快地选择那个期望最大的臂,并计算其与期望最大臂实际给出的奖励的差值作为遗憾.

现在我们的目标更复杂了:我们知道臂的统计参数是变化的!!! 我们显然不能指望选择总选择那个采样平均最大的臂了.

所以这里的策略也应该改变: 它是一种选择序列,虽然UCB最终也是给出一个序列,但是那个序列最终是`收敛`的,而现在的序列是`不收敛`的.

> 定义:策略序列和收益
> 策略A给定一个选择序列$\vec{j}=(j_1,\ldots,j_t)$,在$t$轮后总的收益是:
$$
G_A(t)=G_{\vec{j}}(t)=\sum_s^t X_{j_s}(s)
$$


那么我们用什么来作为理想状态呢? 当然是我们知道全部的奖励结果时.但是显然, 这是的遗憾太强了, 想想看,我们在静态时(UCB)都没这么做.

所以我们需要定义一个弱的遗憾,类似于UCB(同一个作者提出的嘛),我们定义理想状态是那个最佳单操作,即那些最蠢的操作里面最好的那个操作(总选择一个臂).

> 定义:弱遗憾$R_A(T)$为: 
$$
R_A(T)=G_{\max}(T)-G_A(T)=(\max\sum_{t=1}^T X_i(t))-\mathbb(G_A(T))
$$


## EXP3
为什么叫EXP3? 因为是Exponential-weight algorithm for Exploration and Exploitation的缩写,指的是勘探和开发的指数权重算法.

> EXP3算法
> 设置$\gamma\in [0,1]$,初始化权重因子$\omega_i(1)=1, i=1,\ldots,K$
> 每一轮$t$:
>> 1. $p_i(t)=(1-\gamma)\frac{\omega_i(t)}{\sum_{j=1}^{K}\omega_j(t)}+\frac{\gamma}{K}$
>> 2. 根据$p_i(t)$的分布随机生成$i_t$作为下一次选择的臂
>> 3. 定义估计量$ \hat{X}(t) = X_{i_t}(t)/p_{i_t}(t)$
>> 4. 更新选中臂的权重因子$\omega_{i_t}(t+1)=\omega_{i_t}(t)\exp (\gamma \hat{X}_{i_t}(t)/K)$, 其它臂$\omega_j(t+1)=\omega_j(t)$


那么它的弱遗憾是对数增长的:
> 定理
> 对于EXP算法,在任意的轮数$T$下,其遗憾的期望满足
$$
R_A(T)\le (e-1)\gamma G_{\max}(T)+\frac{K\ln K}{\gamma}
$$
证明从略.

这里问题来了,我们应该怎么选择$\gamma$呢?

> 推论
> 如果$G_{\max}(T)\le g$,则$\gamma=\min(1,\sqrt{\frac{K\ln K}{(e-1)g}})$